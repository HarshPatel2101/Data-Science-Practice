{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMcYmPQF3krn",
        "outputId": "4052cf93-844e-43f8-daf0-a89656eb452e"
      },
      "outputs": [],
      "source": [
        "# Libraries needed for NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Libraries needed for Tensorflow processing\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBEuh9TEGBo7"
      },
      "outputs": [],
      "source": [
        "# import our chat-bot intents file\n",
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpFsVWPKGKbI",
        "outputId": "a751eb83-08f0-4c82-e024-3b2bb9a0b865"
      },
      "outputs": [],
      "source": [
        "intents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUNlDhkSGVL1"
      },
      "outputs": [],
      "source": [
        "words = []\n",
        "classes = []\n",
        "documents = []\n",
        "ignore = ['?']\n",
        "# loop through each sentence in the intent's patterns\n",
        "for intent in intents['intents']:\n",
        "    for pattern in intent['patterns']:\n",
        "        # tokenize each and every word in the sentence\n",
        "        w = nltk.word_tokenize(pattern)\n",
        "        # add word to the words list\n",
        "        words.extend(w)\n",
        "        # add word(s) to documents\n",
        "        documents.append((w, intent['tag']))\n",
        "        # add tags to our classes list\n",
        "        if intent['tag'] not in classes:\n",
        "            classes.append(intent['tag'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GO7b9xTwHJMJ",
        "outputId": "58345390-09f4-4d28-ad94-760821ad80a9"
      },
      "outputs": [],
      "source": [
        "# Perform stemming and lower each word as well as remove duplicates\n",
        "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
        "words = sorted(list(set(words)))\n",
        "\n",
        "# remove duplicate classes\n",
        "classes = sorted(list(set(classes)))\n",
        "\n",
        "print (len(documents), \"documents\")\n",
        "print (len(classes), \"classes\", classes)\n",
        "print (len(words), \"unique stemmed words\", words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vziuGlP1Iq-P",
        "outputId": "a322434c-3adf-43ee-ac28-99398b525fa9"
      },
      "outputs": [],
      "source": [
        "#Create training Data\n",
        "training =[]\n",
        "output = []\n",
        "\n",
        "#create an empty array for output\n",
        "output_empty = [0]*len(classes)\n",
        "\n",
        "#create training set,bag of words for each sentence\n",
        "for doc in documents:\n",
        "    #initialize bag of words\n",
        "    bag=[]\n",
        "    #list of tokenized words for the pattern\n",
        "    pattern_words = doc[0]\n",
        "    # streaming each words\n",
        "    pattern_words= [stemmer.stem(word.lower()) for word in pattern_words]\n",
        "    #create bag of words array\n",
        "    for w in words:\n",
        "        bag.append(1) if w in pattern_words else bag.append(0)\n",
        "\n",
        "    #output is '1' for current tag and '0' for rest of other tags\n",
        "    output_row = list(output_empty)\n",
        "    output_row[classes.index(doc[1])] = 1\n",
        "\n",
        "    training.append([bag,output_row])\n",
        "\n",
        "# shuffling features and turning it into np.array\n",
        "random.shuffle(training)\n",
        "training = np.array(training)\n",
        "\n",
        "# creating training list\n",
        "\n",
        "train_x = list(training[:,0])\n",
        "train_y = list(training[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "42dWr6eG_xs0"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Dense(10,input_shape=(len(train_x[0]),)))\n",
        "model.add(tf.keras.layers.Dense(10))\n",
        "model.add(tf.keras.layers.Dense(len(train_y[0]), activation='softmax'))\n",
        "model.compile(tf.keras.optimizers.Adam(), loss='categorical_crossentropy', metrics=['accuracy']) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8fPUE4kBID7",
        "outputId": "9bbaaa7b-9d7c-46a3-cd85-e67d9460658d"
      },
      "outputs": [],
      "source": [
        "model.fit(np.array(train_x), np.array(train_y), epochs=1000, batch_size=8, verbose=1)\n",
        "model.save(\"model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ha2RGmK1Zz5l"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "pickle.dump( {'words':words, 'classes':classes}, open( \"training_data\", \"wb\" ) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIrMS64WCVhm"
      },
      "outputs": [],
      "source": [
        "from keras.models import load_model\n",
        "model = load_model(\"model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ux9WbDZvbAzf"
      },
      "outputs": [],
      "source": [
        "# restoring all the data structures\n",
        "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
        "words = data['words']\n",
        "classes = data['classes']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ms17AFyEbEjt"
      },
      "outputs": [],
      "source": [
        "with open('intents.json') as json_data:\n",
        "    intents = json.load(json_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VB27k_vQbhu4"
      },
      "outputs": [],
      "source": [
        "def clean_up_sentence(sentence):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = nltk.word_tokenize(sentence)\n",
        "    # stemming each word\n",
        "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
        "    return sentence_words\n",
        "\n",
        "# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
        "def bow(sentence, words):\n",
        "    # tokenizing the pattern\n",
        "    sentence_words = clean_up_sentence(sentence)\n",
        "    # generating bag of words\n",
        "    bag = [0]*len(words)  \n",
        "    for s in sentence_words:\n",
        "        for i,w in enumerate(words):\n",
        "            if w == s: \n",
        "                bag[i] = 1\n",
        "    bag=np.array(bag)\n",
        "    return(bag)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lToEtkTb5Pr"
      },
      "outputs": [],
      "source": [
        "ERROR_THRESHOLD = 0.30\n",
        "def classify(sentence):\n",
        "    # generate probabilities from the model\n",
        "    bag = bow(sentence, words)\n",
        "    results = model.predict(np.array([bag]))\n",
        "    # filter out predictions below a threshold\n",
        "    results = [[i,r] for i,r in enumerate(results[0]) if r>ERROR_THRESHOLD]\n",
        "    # sort by strength of probability\n",
        "    results.sort(key=lambda x: x[1], reverse=True)\n",
        "    return_list = []\n",
        "    for r in results:\n",
        "        return_list.append((classes[r[0]], r[1]))\n",
        "    # return tuple of intent and probability\n",
        "    return return_list\n",
        "\n",
        "def response(sentence):\n",
        "    results = classify(sentence)\n",
        "    # if we have a classification then find the matching intent tag\n",
        "    if results:\n",
        "        # loop as long as there are matches to process\n",
        "        while results:\n",
        "            for i in intents['intents']:\n",
        "                # find a tag matching the first result\n",
        "                if i['tag'] == results[0][0]:\n",
        "                    # a random response from the intent\n",
        "                    return print(random.choice(i['responses']))\n",
        "\n",
        "            results.pop(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xp7OJvpHKXuw",
        "outputId": "6ce01353-b906-46d0-881c-eedcfb15c9a3"
      },
      "outputs": [],
      "source": [
        "response('Where are you located?')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PNLGpl0gKCF",
        "outputId": "512775f7-68f1-44f5-f7e0-b533dbdc1ae3"
      },
      "outputs": [],
      "source": [
        "response('That is helpful')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wzeIGchZK2fJ",
        "outputId": "43ccd6cd-5bd7-4f8e-c4ce-64456962450d"
      },
      "outputs": [],
      "source": [
        "response('Bye')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUzPSymMida8",
        "outputId": "754a592a-be2b-4d38-e0fb-038870d1a160"
      },
      "outputs": [],
      "source": [
        "response(\"Who are you?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11MehCNQinZB"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "Chatbot using NLP for Beginners",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
